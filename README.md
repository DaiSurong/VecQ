# VecQ
### Introduce
In this work, we propose a new method, VecQ, which uses vectorized quantization loss for the vectorized weight data, aiming to minimize the quantization loss to achieve better model accuracy and higher computing efficiency. In addition, in order to speed up the proposed quantization process during the model training, we accelerate the quantization process with a parameterized probability estimation method and template-based derivation calculation. Experimental evaluations on the MNIST, CIFAR and ImageNet data sets with numerical DNN models demonstrate that the proposed quantization solution is more effective than state-of-the-art approaches yet with more flexible bitwidth support. Moreover, the evaluation of our quantized models on Saliency Object Detection (SOD) tasks maintains comparable feature extraction quality with up to 16x weight size reduction. 
